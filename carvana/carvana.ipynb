{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e58cdd",
   "metadata": {
    "papermill": {
     "duration": 0.00997,
     "end_time": "2023-02-04T04:40:20.300376",
     "exception": false,
     "start_time": "2023-02-04T04:40:20.290406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Carvana Image Masking Challenge\n",
    "- https://www.kaggle.com/competitions/carvana-image-masking-challenge\n",
    "- ref. https://www.kaggle.com/code/nandwalritik/u-net-pytorch\n",
    "- ref. https://www.kaggle.com/code/yousefradwanlmao/carvana-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18112e30",
   "metadata": {
    "papermill": {
     "duration": 0.023119,
     "end_time": "2023-02-04T04:40:20.330726",
     "exception": false,
     "start_time": "2023-02-04T04:40:20.307607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949f49d",
   "metadata": {
    "papermill": {
     "duration": 0.013608,
     "end_time": "2023-02-04T04:40:20.350257",
     "exception": false,
     "start_time": "2023-02-04T04:40:20.336649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c carvana-image-masking-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726b6b3",
   "metadata": {
    "papermill": {
     "duration": 1.138164,
     "end_time": "2023-02-04T04:40:21.494199",
     "exception": false,
     "start_time": "2023-02-04T04:40:20.356035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0067ae9",
   "metadata": {
    "papermill": {
     "duration": 1.193377,
     "end_time": "2023-02-04T04:40:22.698630",
     "exception": false,
     "start_time": "2023-02-04T04:40:21.505253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6f1d6",
   "metadata": {
    "papermill": {
     "duration": 2.166023,
     "end_time": "2023-02-04T04:40:24.874224",
     "exception": false,
     "start_time": "2023-02-04T04:40:22.708201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263bfff",
   "metadata": {
    "papermill": {
     "duration": 0.005971,
     "end_time": "2023-02-04T04:40:24.886578",
     "exception": false,
     "start_time": "2023-02-04T04:40:24.880607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 定数群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f99822",
   "metadata": {
    "papermill": {
     "duration": 0.077368,
     "end_time": "2023-02-04T04:40:24.970174",
     "exception": false,
     "start_time": "2023-02-04T04:40:24.892806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "LEARNING_RATE = 1e-4\n",
    "SPLIT=0.2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 572\n",
    "IMAGE_WIDTH = 572\n",
    "# PIN_MEMORY = True\n",
    "# BASE_DIR = os.path.abspath(os.path.curdir)\n",
    "BASE_DIR = '.'\n",
    "SRC_PATH = '../input/carvana-image-masking-challenge/'\n",
    "DATAPATH = os.path.join(BASE_DIR, 'data')\n",
    "# TRAIN_IMG_DIR = os.path.join(DATAPATH, 'train')\n",
    "TRAIN_IMG_DIR = os.path.join(DATAPATH, 'train')\n",
    "# TRAIN_MASK_DIR = os.path.join(DATAPATH, 'train_masks')\n",
    "TRAIN_MASK_DIR = os.path.join(DATAPATH, 'train_masks')\n",
    "\n",
    "# VAL_IMG_DIR = \n",
    "# VAL_MASK_DIR = \n",
    "\n",
    "TEST_IMG_DIR = os.path.join(DATAPATH, 'test')\n",
    "TEST_BATCH_SIZE = 16\n",
    "\n",
    "RUN_TRAINING = False\n",
    "SAVE_PATH = os.path.join(BASE_DIR, 'models')\n",
    "MODEL_NAME = \"model.pt\"\n",
    "MODEL_PATH = os.path.join(SAVE_PATH, MODEL_NAME)\n",
    "# MODEL_PATH = os.path.join(\"/kaggle/input/datasets-for-carvana/\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848097f",
   "metadata": {
    "papermill": {
     "duration": 0.006201,
     "end_time": "2023-02-04T04:40:24.982720",
     "exception": false,
     "start_time": "2023-02-04T04:40:24.976519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e4352",
   "metadata": {
    "papermill": {
     "duration": 209.533552,
     "end_time": "2023-02-04T04:43:54.522390",
     "exception": false,
     "start_time": "2023-02-04T04:40:24.988838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "if RUN_TRAINING:\n",
    "    dirs = ['train.zip','train_masks.zip', 'test.zip', 'metadata.csv.zip', 'sample_submission.csv.zip']\n",
    "else:\n",
    "    # dirs = ['test.zip']\n",
    "    dirs = ['train.zip','train_masks.zip', 'test.zip', 'metadata.csv.zip', 'sample_submission.csv.zip']\n",
    "\n",
    "for x in dirs:\n",
    "    bn = os.path.splitext(os.path.basename(x))[0]\n",
    "    d = os.path.join(DATAPATH, bn)\n",
    "    \n",
    "    if os.path.exists(d):\n",
    "        print('skip extract', {bn})\n",
    "        continue\n",
    "        \n",
    "    with zipfile.ZipFile(os.path.join(SRC_PATH, x),'r') as z:\n",
    "        z.extractall(DATAPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7af667",
   "metadata": {
    "papermill": {
     "duration": 0.006083,
     "end_time": "2023-02-04T04:43:54.535211",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.529128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Datasetクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09d7e0",
   "metadata": {
    "papermill": {
     "duration": 0.01924,
     "end_time": "2023-02-04T04:43:54.560576",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.541336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self,images,image_dir,mask_dir=None,transform=None, train=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.isTrain = train\n",
    "        self.images = images\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = os.path.join(self.image_dir,self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.isTrain:\n",
    "            mask_path = os.path.join(self.mask_dir,self.images[index].replace(\".jpg\",\"_mask.gif\"))\n",
    "            mask = np.array(Image.open(mask_path).convert(\"L\"),dtype=np.float32)\n",
    "            mask[mask == 255.0] = 1.0\n",
    "        \n",
    "        if self.transform and self.isTrain:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image, mask = transformed['image'], transformed['mask']\n",
    "        elif self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        if self.isTrain:\n",
    "            return {\"name\":os.path.basename(img_path), \"image\":image, \"mask\":mask}\n",
    "        else:\n",
    "            return {'name': os.path.basename(img_path), 'image': image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87c8b2",
   "metadata": {
    "papermill": {
     "duration": 0.014334,
     "end_time": "2023-02-04T04:43:54.581053",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.566719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images = os.listdir(TRAIN_IMG_DIR)\n",
    "train_masks = os.listdir(TRAIN_MASK_DIR)\n",
    "\n",
    "img = np.array(Image.open(TRAIN_IMG_DIR+\"/\"+train_images[0]).convert(\"RGB\"))\n",
    "plt.imshow(img,cmap=\"gray\")\n",
    "print(img.shape)\n",
    "\n",
    "msk = np.array(Image.open(TRAIN_MASK_DIR+\"/\"+train_images[0].replace(\".jpg\",\"_mask.gif\")).convert(\"L\"))\n",
    "plt.imshow(msk,cmap=\"gray\")\n",
    "print(msk.shape)\n",
    "\n",
    "original_shape = msk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99958690",
   "metadata": {
    "papermill": {
     "duration": 0.007335,
     "end_time": "2023-02-04T04:43:54.594524",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.587189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa02bc",
   "metadata": {
    "papermill": {
     "duration": 0.025767,
     "end_time": "2023-02-04T04:43:54.626376",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.600609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def double_conv(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return conv.to(DEVICE)\n",
    "\n",
    "\n",
    "# def crop_img(tensor, target_tensor):\n",
    "#     target_size = target_tensor.size()[2]\n",
    "#     tensor_size = tensor.size()[2]\n",
    "#     delta = tensor_size-target_size\n",
    "#     delta = delta//2\n",
    "#     # all batch, all channels, heightModified,widthModified\n",
    "\n",
    "#     return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "\n",
    "def addPadding(srcShapeTensor, tensor_whose_shape_isTobechanged):\n",
    "\n",
    "    if(srcShapeTensor.shape != tensor_whose_shape_isTobechanged.shape):\n",
    "        target = torch.zeros(srcShapeTensor.shape)\n",
    "        target[:, :, :tensor_whose_shape_isTobechanged.shape[2],\n",
    "               :tensor_whose_shape_isTobechanged.shape[3]] = tensor_whose_shape_isTobechanged\n",
    "        return target.to(DEVICE)\n",
    "    return tensor_whose_shape_isTobechanged.to(DEVICE)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down_conv_1 = double_conv(3, 64)\n",
    "        self.down_conv_2 = double_conv(64, 128)\n",
    "        self.down_conv_3 = double_conv(128, 256)\n",
    "        self.down_conv_4 = double_conv(256, 512)\n",
    "        self.down_conv_5 = double_conv(512, 1024)\n",
    "\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(\n",
    "            in_channels=1024,\n",
    "            out_channels=512,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.up_conv_1 = double_conv(1024, 512)\n",
    "\n",
    "        self.up_trans_2 = nn.ConvTranspose2d(\n",
    "            in_channels=512,\n",
    "            out_channels=256,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.up_conv_2 = double_conv(512, 256)\n",
    "\n",
    "        self.up_trans_3 = nn.ConvTranspose2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.up_conv_3 = double_conv(256, 128)\n",
    "\n",
    "        self.up_trans_4 = nn.ConvTranspose2d(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.up_conv_4 = double_conv(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=1,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        # expected size\n",
    "        # encoder (Normal convolutions decrease the size)\n",
    "        x1 = self.down_conv_1(image)\n",
    "        # print(\"x1 \"+str(x1.shape))\n",
    "        x2 = self.max_pool_2x2(x1)\n",
    "        # print(\"x2 \"+str(x2.shape))\n",
    "        x3 = self.down_conv_2(x2)\n",
    "        # print(\"x3 \"+str(x3.shape))\n",
    "        x4 = self.max_pool_2x2(x3)\n",
    "        # print(\"x4 \"+str(x4.shape))\n",
    "        x5 = self.down_conv_3(x4)\n",
    "        # print(\"x5 \"+str(x5.shape))\n",
    "        x6 = self.max_pool_2x2(x5)\n",
    "        # print(\"x6 \"+str(x6.shape))\n",
    "        x7 = self.down_conv_4(x6)\n",
    "        # print(\"x7 \"+str(x7.shape))\n",
    "        x8 = self.max_pool_2x2(x7)\n",
    "        # print(\"x8 \"+str(x8.shape))\n",
    "        x9 = self.down_conv_5(x8)\n",
    "        # print(\"x9 \"+str(x9.shape))\n",
    "\n",
    "        # decoder (transposed convolutions increase the size)\n",
    "        x = self.up_trans_1(x9)\n",
    "        x = addPadding(x7, x)\n",
    "        x = self.up_conv_1(torch.cat([x7, x], 1))\n",
    "\n",
    "        x = self.up_trans_2(x)\n",
    "        x = addPadding(x5, x)\n",
    "        x = self.up_conv_2(torch.cat([x5, x], 1))\n",
    "\n",
    "        x = self.up_trans_3(x)\n",
    "        x = addPadding(x3, x)\n",
    "        x = self.up_conv_3(torch.cat([x3, x], 1))\n",
    "\n",
    "        x = self.up_trans_4(x)\n",
    "        x = addPadding(x1, x)\n",
    "        x = self.up_conv_4(torch.cat([x1, x], 1))\n",
    "\n",
    "        x = self.out(x)\n",
    "        # print(x.shape)\n",
    "        return x.to(DEVICE)\n",
    "\n",
    "def post_proc(inputs: torch.tensor, resize: tuple=None) -> np.ndarray:\n",
    "    # print(f'resize: {inputs.size()} -> {resize}')\n",
    "    if resize:\n",
    "        inputs = F.interpolate(inputs, size=resize, mode='bilinear')\n",
    "    output = F.sigmoid(inputs)\n",
    "    output = torch.squeeze(output)\n",
    "    thresh = torch.mean(output)\n",
    "    output[output>thresh] = 1.0\n",
    "    output[output<=thresh]= 0\n",
    "    return output\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     image = torch.rand((3, 3, 572, 572))\n",
    "#     model = UNet()\n",
    "#     print(image.shape)\n",
    "#     model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b70903",
   "metadata": {
    "papermill": {
     "duration": 0.00615,
     "end_time": "2023-02-04T04:43:54.638701",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.632551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 訓練コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bec09d",
   "metadata": {
    "papermill": {
     "duration": 0.019196,
     "end_time": "2023-02-04T04:43:54.664018",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.644822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model,dataloader,data,optimizer,criterion):\n",
    "    print('-------------Training---------------')\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    counter=0\n",
    "    \n",
    "    # num of batches\n",
    "    num_batches = int(len(data)/dataloader.batch_size)\n",
    "    for i,data in tqdm(enumerate(dataloader),total=num_batches):\n",
    "        counter+=1\n",
    "        image,mask = data[\"image\"].to(DEVICE),data[\"mask\"].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        loss = criterion(outputs,mask)\n",
    "        train_running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = train_running_loss/counter\n",
    "    return train_loss\n",
    "\n",
    "def validate(model, dataloader, data, criterion):\n",
    "    print(\"\\n--------Validating---------\\n\")\n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    counter = 0\n",
    "    # number of batches\n",
    "    num_batches = int(len(data)/dataloader.batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i,data in tqdm(enumerate(dataloader),total=num_batches):\n",
    "            counter+=1\n",
    "            image,mask = data[\"image\"].to(DEVICE),data[\"mask\"].to(DEVICE)\n",
    "            outputs = model(image)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs,mask)\n",
    "            valid_running_loss += loss.item()\n",
    "    valid_loss = valid_running_loss/counter\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b786ab6",
   "metadata": {
    "papermill": {
     "duration": 0.014245,
     "end_time": "2023-02-04T04:43:54.684619",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.670374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split(images,splitSize):\n",
    "    imageLen = len(images)\n",
    "    val_len = int(splitSize*imageLen)\n",
    "    train_len = imageLen - val_len\n",
    "    train_images,val_images = images[:train_len],images[train_len:]\n",
    "    return train_images,val_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87c9fd58",
   "metadata": {
    "papermill": {
     "duration": 0.006089,
     "end_time": "2023-02-04T04:43:54.697005",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.690916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac4d5f",
   "metadata": {
    "papermill": {
     "duration": 1.809245,
     "end_time": "2023-02-04T04:43:56.512619",
     "exception": false,
     "start_time": "2023-02-04T04:43:54.703374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb010076",
   "metadata": {
    "papermill": {
     "duration": 0.031511,
     "end_time": "2023-02-04T04:43:56.551262",
     "exception": false,
     "start_time": "2023-02-04T04:43:56.519751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH),\n",
    "    # A.Rotate(limit=35,p=1.0),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.VerticalFlip(p=0.1),\n",
    "    A.Normalize(\n",
    "        mean=[0.0,0.0,0.0],\n",
    "        std = [1.0,1.0,1.0],\n",
    "        max_pixel_value=255.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "validation_transform = A.Compose([\n",
    "    A.Resize(IMAGE_HEIGHT,IMAGE_WIDTH),\n",
    "    A.Normalize(\n",
    "        mean = [0.0,0.0,0.0],\n",
    "        std = [1.0,1.0,1.0],\n",
    "        max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc33b4d-4cd7-4111-adcb-4de342ed6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "train_images = os.listdir(TRAIN_IMG_DIR)\n",
    "train_masks = os.listdir(TRAIN_MASK_DIR)    \n",
    "train_images_path,val_images_path = train_test_split(train_images,SPLIT)\n",
    "train_data = CarvanaDataset(train_images_path,TRAIN_IMG_DIR,TRAIN_MASK_DIR,train_transform,True)\n",
    "valid_data = CarvanaDataset(val_images_path,TRAIN_IMG_DIR,TRAIN_MASK_DIR,validation_transform,True)\n",
    "train_dataloader = DataLoader(train_data,batch_size=TRAIN_BATCH_SIZE, pin_memory=True,shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data,batch_size=TRAIN_BATCH_SIZE, pin_memory=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f804",
   "metadata": {
    "papermill": {
     "duration": 0.006205,
     "end_time": "2023-02-04T04:43:56.564265",
     "exception": false,
     "start_time": "2023-02-04T04:43:56.558060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 訓練の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57936a86",
   "metadata": {
    "papermill": {
     "duration": 0.020106,
     "end_time": "2023-02-04T04:43:56.590822",
     "exception": false,
     "start_time": "2023-02-04T04:43:56.570716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if RUN_TRAINING:\n",
    "    train_loss = []\n",
    "    val_loss =[]\n",
    "    model = UNet().to(DEVICE)\n",
    "    \n",
    "    # using multiple GPU\n",
    "    # ref. https://aru47.hatenablog.com/entry/2020/11/06/225052\n",
    "    if DEVICE == 'cuda' and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1} of {EPOCHS}\")\n",
    "        train_epoch_loss = fit(model, train_dataloader, train_data,optimizer,criterion)\n",
    "        val_epoch_loss = validate(model, valid_dataloader, valid_data, criterion)\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "        print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "        print(f'Val Loss: {val_epoch_loss:.4f}')\n",
    "\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color=\"orange\", label='train loss')\n",
    "    plt.plot(val_loss, color=\"red\", label='validation loss')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    # plt.savefig(f\"../input/loss.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': EPOCHS,\n",
    "        'model_state_dict': model.state_dict() if iftype(model) is torch.nn.DataParallel else model.module.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': criterion,\n",
    "    }, MODEL_PATH)\n",
    "    print(\"\\n---------DONE TRAINING----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f3c68",
   "metadata": {
    "papermill": {
     "duration": 0.00763,
     "end_time": "2023-02-04T04:43:56.606242",
     "exception": false,
     "start_time": "2023-02-04T04:43:56.598612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc3127",
   "metadata": {
    "papermill": {
     "duration": 0.017428,
     "end_time": "2023-02-04T04:43:56.630155",
     "exception": false,
     "start_time": "2023-02-04T04:43:56.612727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref. https://www.kaggle.com/code/zfturbo/baseline-optimal-mask\n",
    "def dice(img1, img2, empty_score=1.0):\n",
    "    im1 = img1.astype(np.bool)\n",
    "    im2 = img2.astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(f\"Shape mismatch: im1 and im2 must have the same shape. [{im1.shape}, {im2.shape}]\")\n",
    "\n",
    "    im_sum = im1.sum() + im2.sum()\n",
    "    if im_sum == 0:\n",
    "        return empty_score\n",
    "\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / im_sum\n",
    "\n",
    "# def get_score(train_masks, avg_mask, thr):\n",
    "#     d = 0.0\n",
    "#     for i in range(train_masks.shape[0]):\n",
    "#         d += dice(train_masks[i], avg_mask)\n",
    "#     return d/train_masks.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73e5cd",
   "metadata": {
    "papermill": {
     "duration": 0.014997,
     "end_time": "2023-02-04T04:43:56.688838",
     "exception": false,
     "start_time": "2023-02-04T04:43:56.673841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# https://qiita.com/conta_/items/c3e173e891145e87e668\n",
    "def fix_model_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k\n",
    "        if name.startswith('module.'):\n",
    "            name = name[7:]  # remove 'module.' of dataparallel\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c0ea6-75c6-4ff0-ac43-21ebe48c4f4b",
   "metadata": {},
   "source": [
    "## ランレングス符号化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3378b9",
   "metadata": {
    "papermill": {
     "duration": 0.017664,
     "end_time": "2023-02-04T04:44:02.216896",
     "exception": false,
     "start_time": "2023-02-04T04:44:02.199232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    " \n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d757700-bcc6-4b48-9b36-28e5f0a69a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_pathes = sorted(list(map(lambda x: os.path.join(TRAIN_IMG_DIR, x), os.listdir(TRAIN_IMG_DIR))))\n",
    "img = np.array(Image.open(train_img_pathes[0]))\n",
    "original_shape = img.shape[:2] # Height, Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea103e46-eab7-4f68-a26b-164d752dfa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' not in locals():\n",
    "    ckpt = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    if type(ckpt) is torch.nn.DataParallel:\n",
    "        ckpt = fix_model_state_dict(ckpt.state_dict())\n",
    "    \n",
    "    model = UNet()\n",
    "    model.load_state_dict(ckpt)\n",
    "    if DEVICE == 'cuda' and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    model.to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = valid_data.__getitem__(0)\n",
    "    name, img, mask = data['name'], data['image'], data['mask'] \n",
    "    print(f'name: {name}')\n",
    "    print(f'image_shape: {img.shape}')\n",
    "    print(f'mask_shape: {mask.shape}')\n",
    "    \n",
    "    img = img.unsqueeze(0).to(DEVICE)\n",
    "    output = model(img)\n",
    "    # output = output.squeeze()\n",
    "    pred = post_proc(output, original_shape[:2])\n",
    "    print(f'pred_shape: {pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953dd49-a6d5-42ce-af78-cc99dc859539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validation encoding\n",
    "# valid_data = CarvanaDataset(val_images_path,TRAIN_IMG_DIR,TRAIN_MASK_DIR, validation,True)\n",
    "data = valid_data.__getitem__(0)\n",
    "ref_name = data['name']\n",
    "ref_img = data['image'].squeeze().detach().cpu().numpy().transpose(1,2,0)\n",
    "ref_mask = data['mask']\n",
    "ref_mask = ref_mask.reshape(1,1, ref_mask.size(0), ref_mask.size(1))\n",
    "ref_mask = F.interpolate(ref_mask, tuple(original_shape[:2]), mode='bilinear')\n",
    "ref_mask = ref_mask.squeeze().detach().cpu().numpy()\n",
    "\n",
    "print(f'pred (min,max): ({torch.min(pred)}:{torch.max(pred)})')\n",
    "print(f'pred shape: {pred.size()}')\n",
    "# print('ref_mask: ', ref_mask.))\n",
    "\n",
    "# pred_img = img.squeeze().detach().cpu().numpy().transpose(1,2,0)\n",
    "pred_img = ref_img\n",
    "pred_mask = pred.detach().cpu().numpy()\n",
    "score = dice(ref_mask, pred_mask)\n",
    "print(f'score: {score}')\n",
    "encoded_mask = rle_encode(pred_mask)\n",
    "print(f'enc_mask: {encoded_mask[:80]}')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "ax[0, 0].imshow(ref_img, cmap=\"gray\", label='train_image')\n",
    "ax[0, 1].imshow(ref_mask, cmap=\"gray\", label='train_mask')\n",
    "ax[1, 0].imshow(pred_img, cmap=\"gray\", label='pred_image')\n",
    "ax[1, 1].imshow(pred_mask, cmap=\"gray\", label='pred_mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350df07-58d1-41ef-820a-aeb4cd3314ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATAPATH, 'train_masks.csv'))\n",
    "df[df.img == data['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcfdb6-488f-43a0-b904-6b09ba599d09",
   "metadata": {},
   "source": [
    "# 評価コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612094c7-07e0-4fc4-a9f4-9a498fbf1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, data, save_preds=False):\n",
    "    print(\"\\n--------Predict Testset---------\\n\")\n",
    "    model.eval()\n",
    "    counter = 0\n",
    "    # number of batches\n",
    "    num_batches = int(len(data)/dataloader.batch_size)\n",
    "    \n",
    "    masks, names, scores = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader),total=num_batches):\n",
    "            counter+=1\n",
    "            # name, image, mask = data['name'], data[\"image\"], data['mask']\n",
    "            name, image = data['name'], data[\"image\"]\n",
    "            # print('image.shape:', image.shape)\n",
    "            # print('mask.shape:', mask.shape)\n",
    "\n",
    "            # image = image.unsqueeze(0).to(DEVICE)\n",
    "            outputs = model(image)\n",
    "            # print('outputs.shape:', outputs.shape)\n",
    "\n",
    "            preds = post_proc(outputs, original_shape[:2])\n",
    "            # preds = post_proc(outputs)\n",
    "            preds = preds.squeeze().detach().cpu().numpy()\n",
    "            # mask = mask.detach().cpu().numpy()\n",
    "            # print('preds.shape: ', preds.shape)\n",
    "            \n",
    "            if save_preds:\n",
    "                save_dir = os.path.join(os.path.join(BASE_DIR, 'preds'))\n",
    "                save_path = list(map(lambda x: os.path.join(save_dir, x + '.npy'), name))\n",
    "                for x in zip(save_path, preds):\n",
    "                    np.save(x[0], x[1])\n",
    "                \n",
    "            # score = list(map(lambda x: dice(x[0], x[1]), zip(preds, mask)))\n",
    "            encoded_mask = list(map(rle_encode, preds))\n",
    "            # plt.imshow(masks[0],cmap=\"gray\")\n",
    "            # print('score: ', score)\n",
    "            \n",
    "            names.append(name)\n",
    "            masks.append(encoded_mask)\n",
    "            # scores.append(score)\n",
    "\n",
    "    # return masks, names, scores\n",
    "    return masks, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822a5a3",
   "metadata": {
    "papermill": {
     "duration": 17279.194616,
     "end_time": "2023-02-04T09:32:01.443384",
     "exception": false,
     "start_time": "2023-02-04T04:44:02.248768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_images_path = os.listdir(TEST_IMG_DIR)\n",
    "test_data = CarvanaDataset(test_images_path, TEST_IMG_DIR, None, validation_transform, False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# score, preds = evaluate(model, test_dataloader, test_data)\n",
    "masks, names = test(model, test_dataloader, test_data, save_preds=False)\n",
    "\n",
    "# for Testing on Single datapoint after training\n",
    "# plt.imshow(np.transpose(np.array(data['image']),(1,2,0)),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ff4ea-1f44-4594-918a-6a95b441a71b",
   "metadata": {},
   "source": [
    "# 推論結果のSubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f20889",
   "metadata": {
    "papermill": {
     "duration": 0.35709,
     "end_time": "2023-02-04T09:32:02.117648",
     "exception": false,
     "start_time": "2023-02-04T09:32:01.760558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_submission(masks, names):\n",
    "    print('Create submission...')\n",
    "    m = itertools.chain.from_iterable(masks)\n",
    "    n = itertools.chain.from_iterable(names)\n",
    "    \n",
    "    # df = pd.read_csv(os.path.join(DATAPATH, 'sample_submission.csv'))\n",
    "    df = pd.DataFrame(data={'img': n, 'rle_mask': m}).sort_values('img')\n",
    "    # df.append({'img':results[0], 'rle_mask':results[1]}, ignore_index=True)\n",
    "    df.drop_duplicates(subset='img', keep='last', ignore_index=True, inplace=True)\n",
    "    df.to_csv('submission.gz', index=False, compression='gzip')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e67f48",
   "metadata": {
    "papermill": {
     "duration": 79.063406,
     "end_time": "2023-02-04T09:33:21.492966",
     "exception": false,
     "start_time": "2023-02-04T09:32:02.429560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_submission(masks, names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt1.7",
   "language": "python",
   "name": "pt1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17593.015269,
   "end_time": "2023-02-04T09:33:25.724260",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-04T04:40:12.708991",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
